{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMp34zuq4aMLAghrTM3HvDa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/traderjoevitamins/librarian/blob/main/librarian2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow_hub\n",
        "!pip install tensorflow_text\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text"
      ],
      "metadata": {
        "id": "mWUChlYus_Lw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n"
      ],
      "metadata": {
        "id": "P7NH0P1Pt5VB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYxxYRs7scVd"
      },
      "outputs": [],
      "source": [
        "encoder_url = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4'\n",
        "preprocess_url = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\n",
        "nyt = pd.read_csv('/content/nyt(3)_embedded_1k_reviews.csv')\n",
        "gr = pd.read_csv('/content/goodreads_embedded_1k_reviews.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gr['openai_embed'] = gr['embedding']\n",
        "gr = gr[['Title', 'Author', 'ISBN13', 'synopsis', 'enjoyed', 'openai_embed']]\n",
        "nyt['openai_embed'] = nyt['embedding']\n",
        "nyt = nyt[['titleleft', 'authors', 'isbn13left', 'synopsis', 'openai_embed']]"
      ],
      "metadata": {
        "id": "MwVWBQmFwgnL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## preprocessed layer\n",
        "bert_preprocess_model = hub.KerasLayer(preprocess_url)"
      ],
      "metadata": {
        "id": "yICAm1zvtR2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gr_bert_preprocess = bert_preprocess_model(gr['synopsis'])\n",
        "nyt_bert_preprocess=  bert_preprocess_model(nyt['synopsis'])"
      ],
      "metadata": {
        "id": "QD7ZfPWk4KF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The shape for gr will be 82 by 128 because we have 82 entries. same goes for the number of entries in nyt at the time of viewing. 128 is the max of sentence. Each array corrosponds to a sentence.\n",
        "\n",
        "input mask notes\n",
        "- CLS token in the begining of the sentence SEP token in the end of the sentence\n",
        "\n",
        "input type ids\n",
        "- all 0's but this is normal because 0 represents context and 1 represents questions\n",
        "\n",
        "input word ids\n",
        "- CLS = 101\n",
        "- SEP = 102\n",
        "\n",
        "gr_bert_preprocess.keys() \\\n",
        "nyt_bert_preprocess.keys()\n",
        "\n"
      ],
      "metadata": {
        "id": "Qvta6hIqy2PI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## encoder layer\n",
        "bert_model = hub.KerasLayer(encoder_url)\n",
        "gr_bert_embed = bert_model(gr_bert_preprocess)\n",
        "nyt_bert_embed = bert_model(nyt_bert_preprocess)"
      ],
      "metadata": {
        "id": "ri4lU_b43F39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "embeddings /\n",
        "encoder outputs\n",
        "- len of 12 because model is small\n",
        "- output of each indvidual encoder\n",
        "- last encoder output is literally the sequence output\n",
        "\n",
        "sequence output\n",
        "- first number = the number of sentences\n",
        "- second number = 128\n",
        "- third number = 728 size vector for each word\n",
        "\n",
        "pooled output\n",
        "- embedding for entire sentence\n",
        "\n",
        "default\n",
        "-"
      ],
      "metadata": {
        "id": "r8gj6kcf66-R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gr['bert_embed_pooled'] = [arr for arr in gr_bert_embed['pooled_output']]\n",
        "nyt['bert_embed_pooled'] = [arr for arr in nyt_bert_embed['pooled_output']]\n",
        "\n",
        "gr['bert_embed_seq'] = [arr for arr in gr_bert_embed['sequence_output']]\n",
        "nyt['bert_embed_seq'] = [arr for arr in nyt_bert_embed['sequence_output']]"
      ],
      "metadata": {
        "id": "LOI-Y0L-AEUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Time to Experiment!**\n",
        "We are going to see if individual word embeddings (sequence) give different predictions than sentence embeddings (pooled)\n",
        "\n",
        "independent variables: \\\n",
        "open ai embeddings \\\n",
        "sequence output (bert) \\\n",
        "pooled output (bert) \\"
      ],
      "metadata": {
        "id": "rAz2Z1wvBndX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install tensorflow_decision_forests\n",
        "import tensorflow_decision_forests as tfdf\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import math"
      ],
      "metadata": {
        "id": "M3S-SkLi-qQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gr.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFXT61IbVM1y",
        "outputId": "a97825dc-db7a-4c2a-e8c1-798e7d73cdee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Title', 'Author', 'ISBN13', 'synopsis', 'enjoyed', 'openai_embed',\n",
              "       'bert_embed', 'bert_embed_seq', 'bert_embed_pooled'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tfgrseq = tf.convert_to_tensor([arr for arr in gr_bert_embed['sequence_output']])\n",
        "tf_gr = tfdf.keras.pd_dataframe_to_tf_dataset((gr[['Title', 'Author', 'ISBN13', 'synopsis', 'enjoyed', 'openai_embed']],tfgrseq), label = 'enjoyed')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        },
        "id": "CoKEDSZjJ-H-",
        "outputId": "e13f8cc9-6b08-4bf7-f7a2-bc1d0363f3ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-83-aff8118cd47e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtfgrseq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0marr\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgr_bert_embed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sequence_output'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtf_gr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpd_dataframe_to_tf_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Title'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Author'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ISBN13'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'synopsis'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'enjoyed'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'openai_embed'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtfgrseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'enjoyed'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow_decision_forests/keras/core_inference.py\u001b[0m in \u001b[0;36mpd_dataframe_to_tf_dataset\u001b[0;34m(dataframe, label, task, max_num_classes, in_place, fix_feature_names, weight, batch_size)\u001b[0m\n\u001b[1;32m   1089\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0min_place\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1091\u001b[0;31m     \u001b[0mdataframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1092\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1093\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'copy'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## turning pd dataframes into tensor dataframes\n",
        "## ok so turn the numpy array stored in the df to a tf tensor\n",
        "\n",
        "#tf_gr = tfdf.keras.pd_dataframe_to_tf_dataset(gr[['Title', 'Author', 'ISBN13', 'synopsis', 'enjoyed', 'openai_embed']], label = 'enjoyed')\n",
        "#tf_nyt = tfdf.keras.pd_dataframe_to_tf_dataset(nyt[['titleleft', 'authors', 'isbn13left', 'synopsis', 'openai_embed']])\n",
        "#tensor_pooled = tf.convert_to_tensor([i for i in nyt['bert_embed_pooled']])\n",
        "#tensor_seq = tf.convert_to_tensor([i for i in nyt['bert_embed_seq']])\n",
        "tensor_seq2 = nyt['bert_embed_seq']\n",
        "\n",
        "#tf_nyt['bert_embed_pooled', 'bert_embed_seq'] =\n",
        "\n"
      ],
      "metadata": {
        "id": "9kLLU2UZJxvR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YV1bgj5BOJKz",
        "outputId": "faeb7f16-871f-4ba7-9de3-70dd7df6fa63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=({'Title': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'Author': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'ISBN13': TensorSpec(shape=(None,), dtype=tf.int64, name=None), 'synopsis': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'openai_embed': TensorSpec(shape=(None,), dtype=tf.string, name=None)}, TensorSpec(shape=(None,), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "wUR31Ah0Oeuu",
        "outputId": "86e587b8-243b-4f12-d74a-1b12fcda245a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-74-ef93a83ee564>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtf_nyt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: '_PrefetchDataset' object has no attribute 'head'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#tf_nyt['bert_embed_pooled'] = tensor_pooled\n",
        "tf_nyt['bert_embed_seq'] = tensor_seq2\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "QM-mhJ5kMtkv",
        "outputId": "034fc1a9-6b4f-4bb3-d2c7-0733ef3fbf24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-72-a0b69e0d2356>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#tf_nyt['bert_embed_pooled'] = tensor_pooled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtf_nyt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bert_embed_seq'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_seq2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: '_PrefetchDataset' object does not support item assignment"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "O2zKib0Fxgnw"
      }
    }
  ]
}